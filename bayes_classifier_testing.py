# -*- coding: utf-8 -*-
"""Bayes Classifier - Testing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h2deW7aFjXdbQ59jtxaUuyXc_N1ssOW3

# Notebook Imports
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
# from matplotlib.colors import ListedColormap
import seaborn as sns

# from sklearn.preprocessing import StandardScaler

from google.colab import drive
drive.mount('/content/drive')

"""# Constants"""

TOKEN_SPAM_PROB_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/03_Testing/prob-spam.txt'
TOKEN_HAM_PROB_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/03_Testing/prob-nonspam.txt'
TOKEN_ALL_PROB_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/03_Testing/prob-all.txt'

TEST_FEATURE_MATRIX = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/03_Testing/test-features.txt'
TEST_TARGET_MATRIX = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/03_Testing/test-target.txt'

VOCAB_SIZE = 2500

"""# Load the Data"""

# Features
X_test = np.loadtxt(TEST_FEATURE_MATRIX, delimiter=' ')

# Target
y_test = np.loadtxt(TEST_TARGET_MATRIX, delimiter=' ')

# Token Probabilities
prob_token_spam = np.loadtxt(TOKEN_SPAM_PROB_FILE, delimiter=' ')
prob_token_ham = np.loadtxt(TOKEN_HAM_PROB_FILE, delimiter=' ')
prob_token_all = np.loadtxt(TOKEN_ALL_PROB_FILE, delimiter=' ')

X_test

y_test

prob_token_spam

prob_token_ham

prob_token_all

"""# Calculating the Joint Probability

## The Dot Product
"""

a = np.array([1, 2, 3])
b = np.array([0, 5, 4])

print('a = ', a)
print('b = ', b)

a.shape

a.dot(b)

X_test.shape

prob_token_spam.shape

X_test.shape

prob_token_spam.shape

print('shape of dot product is', X_test.dot(prob_token_spam).shape)

"""## Set the Prior Probability: Spam

$$P(Spam \, | \, X) = \frac{P(X \, | Spam) \,  P(Spam)} {P(X)}$$
"""

# we acquired this is in our training file
PROB_SPAM = 0.3116 
PROB_HAM = 1 - PROB_SPAM

# we will use log likelihoods

log_likelihood_token_spam = np.log(prob_token_spam)
log_likelihood_token_ham = np.log(prob_token_ham)
log_likelihood_token_all = np.log(prob_token_all)

log_likelihood_token_spam[:5]

"""## Joint Probability in log format"""

# LL = Log Likelihood

# joint probability of spam email = 
# (dot product of X_test and (LL of token|spam - LL of token|all emails)) + LL(Spam|All emails)

# we are subtracting instead of dividing because we are dealing with logs 

joint_log_spam = X_test.dot(log_likelihood_token_spam - log_likelihood_token_all) + np.log(PROB_SPAM)

joint_log_spam[-5:]

joint_log_spam.shape

"""## Set the Prior Probability: Non-Spam

$$P(Ham \, | \, X) = \frac{P(X \, | Ham) \,  P(Ham)} {P(X)}$$
"""

joint_log_ham = X_test.dot(log_likelihood_token_ham - log_likelihood_token_all) + np.log(PROB_HAM)

joint_log_ham[:5]

joint_log_ham.size

joint_log_ham.shape

"""# Making Predictions 

## Checking for the higher Joint Probability

$$P(Spam \, | \, X) > P(Ham \, | \, X)$$
<center>OR<center>

$$P(Spam \, | \, X) < P(Ham \, | \, X)$$

Create a vector of predictions ($\hat y$)

Spam = 1
<br>
Non-Spam = 0
"""

# prediction[i] = True 
#    if log_spam > log_ham

# prediction[i] = False 
#    if log_spam < log_ham
prediction = joint_log_spam > joint_log_ham

prediction[-5:]

y_test[-5:]

"""Simplify our expressions

Recall: We are subtracting instead of dividing because we took the log probabities

joint_log_spam = X_test.dot(log_likelihood_token_spam - log_likelihood_token_all) + np.log(PROB_SPAM)

is equivalent to the naive bayes equation


we can simplify our expression by removing the likelihood of a token occuring given all words(log_likelihood_token_all) as the probability of an email being spam or not spam doesn't depend on it.
"""

# joint_log_spam = X_test.dot(log_likelihood_token_spam) + np.log(PROB_SPAM)
# joint_log_ham = X_test.dot(log_likelihood_token_ham) + np.log(PROB_HAM)

"""# Metrics and Accuracy

## Accuracy
"""

# sum up all our correct predictions

# correct predictins are where y_test is equal to our predictions

correct_pred = (y_test == prediction).sum()
incorrect_pred = (y_test != prediction).sum()

total_emails = X_test.shape[0]
accuracy = correct_pred / total_emails

print("The number of correct predictions are: ", correct_pred)
print("The number of incorrect predictions are: ", incorrect_pred)
print("Accuracy of Naive Bayes Classifier is: {:.2%}".format(accuracy))

"""Warning! Accuracy is inutuitive and easy to understand but for an imbalanced data set, it may give us a misleadingly high number. To verify the quality or our results we will visualize our results as well as use other metrics such as Recall, Precision, and F score.

# Visualizing Our Results
"""

# Chart Styling Info 

xaxis_label = 'P(X|Nonspam)'
yaxis_label = 'P(X|Spam)'

linedata = np.linspace(start=-175, stop=25, num=1000)

plt.figure(figsize=(11, 7))
plt.xlabel(xaxis_label, fontsize=14)
plt.ylabel(yaxis_label, fontsize=14)

# set scale for x and y axis
# plt.xlim([-1000, 1000])
# plt.ylim([-1000, 1000])
plt.scatter(joint_log_ham, joint_log_spam)

plt.style.use('dark_background')


plt.show()

# Chart Styling

sns.set_style('whitegrid')
labels = 'Actual Category'

summary_df = pd.DataFrame(
    {yaxis_label: joint_log_spam,
     xaxis_label: joint_log_ham,
     labels: y_test}
)

# plt.figure(figsize=(16, 10))
custom_colors = ['#003566', '#e5383b']
sns.lmplot(x=xaxis_label,
           y=yaxis_label,
           data=summary_df,
           height=6.5,
           fit_reg=False,
           legend=False,
           scatter_kws={'alpha': 0.3, 's': 25},
           hue=labels,
           markers=['o', 'x'],
           palette=custom_colors)

# plt.xlim([-1000, 1])
# plt.ylim([-1000, 1])
plt.plot(linedata, linedata, color='black')
plt.legend(('Decision Boundary', 'Nonspam', 'Spam'),
           loc='lower right',
           fontsize=14)
plt.show()

"""# False Positives and False Negatives"""

'''
FP: spam classifier classifies email as spam but is non-spam
FN: spam classifier classifies email as non-spam but is spam
'''

# return counts of spam classified and non-spam classifed
np.unique(prediction, return_counts=True)

# bitwise operator: &

# allows to compare element by element

true_pos = (y_test == 1) & (prediction == 1)

true_pos.sum() # number of actual true spam emails

# number of emails that were actually spam but were classified non-spam
false_pos = (y_test == 0) & (prediction == 1)

false_pos.sum()

# number of emails that are non-spam and were classified as non-spam correctly
true_neg = (y_test == 0) & (prediction == 0)

true_neg.sum()

# number of emails that were actually non-spam but were classified as spam
false_neg = (y_test == 1) & (prediction == 0)

false_neg.sum()

"""# Recall Score (Sensitivity)

$$Recall = \frac{True \, Positives} {True \, Positives \, + False \, Negatives}$$

The lower the number of False Negatives, the higher our recall score.

In our case, our recall score will tell us the rate at which we CORRECTLY identified spam emails given the number of emails that are ACTUALLY spam.
"""

recall_score = true_pos.sum() / (true_pos.sum() + false_neg.sum())

print("The recall score is {:.2%}".format(recall_score))

"""# Precision (Positive Predictive Value)

$$Precision = \frac{True \, Positives} {True \, Positives \, + False \, Positives}$$

In our case, our precision score will tell us the rate at which we CORRECTLY identified spam emails out of all the emails that were PREDICTED to be spam.
"""

precision_score = true_pos.sum() / (true_pos.sum() + false_pos.sum())

print("The precision score is {:.2%}".format(precision_score))

"""# F-Score (F1 Metric)

$$F-Score = 2 \times \frac{Precision \, \times Recall} {Precision \, + Recall}$$
<br>
$$0 \leq F-Score \leq 1$$

In mathematical terms, the F-Score is the harmonic average of the precision and recall.

Reconcile Recall and Precision with F-Score
"""

f_score = 2 * (precision_score * recall_score) / (precision_score + recall_score)

print("The F-Score is {:.2}".format(f_score))