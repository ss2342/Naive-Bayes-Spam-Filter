# -*- coding: utf-8 -*-
"""Bayes Classifier - Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11gLJfcE0LJcXkZjB2oEx6_lQQhlxllMR

# Notebook Imports
"""

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

"""# Constants"""

TRAINING_DATA_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/02_Training/train-data.txt'
TEST_DATA_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/02_Training/test-data.txt'

TOKEN_SPAM_PROB_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/03_Testing/prob-spam.txt'
TOKEN_HAM_PROB_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/03_Testing/prob-nonspam.txt'
TOKEN_ALL_PROB_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/03_Testing/prob-all.txt'

TEST_FEATURE_MATRIX = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/03_Testing/test-features.txt'
TEST_TARGET_MATRIX = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/03_Testing/test-target.txt'

VOCAB_SIZE = 2500

"""# Read and Load Features from .txt files into Numpy arrays"""

# load and save training data into numpy arrays

sparse_train_data = np.loadtxt(TRAINING_DATA_FILE, # name of data file
                               delimiter=' ', # character that sets boundaries between plain text data, we are using a single whitespace
                               dtype=int) # type of character, in our case, its integer

sparse_train_data.shape

# load and save test data into numpy arrays

sparse_test_data = np.loadtxt(TEST_DATA_FILE, # name of data file
                               delimiter=' ', # character that sets boundaries between plain text data, we are using a single whitespace
                               dtype=int) # type of character, in our case, its integer

"""My data here is a bit messed up so you can ignore the next few cells where I am fixing the order in my test numpy array"""

sparse_test_data.shape

sparse_train_data[:5]

sparse_train_data[-5:]

sparse_test_data[:5]

sparse_test_data = np.delete(sparse_test_data, 0, 1)

sparse_test_data

sparse_test_data[:, [0, 1]] = sparse_test_data[:, [1, 0]]

sparse_test_data[:, [1, 3]] = sparse_test_data[:, [3, 1]]

sparse_test_data[:, [2, 3]] = sparse_test_data[:, [3, 2]]

sparse_test_data

sparse_test_data.shape

"""Everything is okay now!"""

print("Number of Rows in training file: ", sparse_train_data.shape[0])
print("Number of Rows in test file: ", sparse_test_data.shape[0])

print("Number of emails in training file", np.unique(sparse_train_data[:, 0]).size)
print("Number of emails in test file", np.unique(sparse_test_data[:, 0]).size)

"""# From Sparse Matrix to Full Matrix

## Initialize an Empty DataFrame
"""

column_names = ['DOC_ID'] + ['CATEGORY'] + list(range(0, VOCAB_SIZE))

column_names[:5]

len(column_names) # should be 2500

index_names = np.unique(sparse_train_data[:, 0])

index_names

full_train_data = pd.DataFrame(index=index_names,
                               columns=column_names)

full_train_data.fillna(value=0, inplace=True) # fill all empty cells with 0

full_train_data.head()

"""## Create Full Matrix from Sparse Matrix"""

def make_full_matrix(sparse_matrix, num_of_words, doc_idx=0, 
                     word_idx=1, cat_idx=2, freq_idx=3):
  
  '''
  Form a full matrix from a sparse matrix

  Keyword arguments:

  sparse_matrix: REQUIRED
                 numpy array sparse matrix

  num_of_words: REQUIRED
                max size of our vocab data (i.e. 2500)

  doc_idx: OPTIONAL
           position of doc_id column in sparse matrix
           Default: doc_idx=0

  word_idx: OPTIONAL
            position of word_id column in sparse matrix
            Default: word_idx=1

  cat_idx: OPTIONAL
           position of cat_id column in sparse matrix
           Default: cat_idx=2

  freq_idx: OPTIONAL
            position of occurence column in sparse matrix
            Default: freq_idx=3

  Return value: pandas DataFrame
  '''

  # first initialize an empty full matrix dataFrame 
  column_names = ['DOC_ID'] + ['CATEGORY'] + list(range(0, VOCAB_SIZE))
  doc_id_names = np.unique(sparse_matrix[:, 0])

  full_matrix = pd.DataFrame(index=doc_id_names, columns=column_names)
  full_matrix.fillna(value=0, inplace=True)


  for i in range(sparse_matrix.shape[0]): # going row by row

    doc_id = sparse_matrix[i][doc_idx]  # [row i][column doc_idx] 
    word_id = sparse_matrix[i][word_idx] # [row i][column word_idx] 
    label = sparse_matrix[i][cat_idx] # [row i][column cat_idx]
    occurence = sparse_matrix[i][freq_idx] # [row i][column freq_idx]

    # row number will correspond to doc_id
    full_matrix.at[doc_id, 'DOC_ID'] = doc_id # doc_id will go under DOC_ID column
    full_matrix.at[doc_id, 'CATEGORY'] = label # label will go under CATEGORY column
    full_matrix.at[doc_id, word_id] = occurence # occurence will go under WORD_ID column

  full_matrix.set_index('DOC_ID', inplace=True) # set index
  return full_matrix

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# full_train_data = make_full_matrix(sparse_matrix=sparse_train_data,
#                                    num_of_words=VOCAB_SIZE)

full_train_data.head()

full_train_data.CATEGORY[5789]

"""# Training the Naive Bayes Model

## Calculating the Probability of Spam
"""

# Calculating probability of spam

ham_count = full_train_data.CATEGORY.value_counts()[0] # total non-spam emails
spam_count = full_train_data.CATEGORY.value_counts()[1] # total spam emails
total_count = full_train_data.CATEGORY.size # total emails

# spam / (spam + non-spam)

prob_spam = spam_count / total_count

prob_spam

"""## Total Number of Words (Tokens)"""

# select all features except CATEGORY
full_train_features = full_train_data.loc[:, full_train_data.columns != 'CATEGORY']

full_train_features

# Way 1) column wise sum

full_train_features[1].sum()

total_words = 0
total_words_in_spam = 0

for i in range(VOCAB_SIZE):

  total_words += full_train_features[i].sum()

print(total_words)

# Way 2) row wise sum

# sum across columns
emails_length = full_train_features.sum(axis=1)

emails_length.shape

emails_length[:5]

# output

'''
output

0  tokenCount or row 0
1  tokenCount or row 1
2  tokenCount or row 2
3  tokenCount or row 3
.  tokenCount or row 4
.  tokenCount or row 5
n  tokenCount or row n
'''

total_words2 = emails_length.sum()

total_words2

spam_emails_length = full_train_data[full_train_data.CATEGORY == 1].sum(axis=1)

ham_emails_length = full_train_data[full_train_data.CATEGORY == 0].sum(axis=1)

spam_emails_length.shape

total_words_in_spam = spam_emails_length.sum()

total_words_in_spam

'''Since we already know the total number of words and 
   total number of words in spam emails, we can just subtract the two
   to get the total number of words in non-spam emails'''

total_words_in_ham = total_words - total_words_in_spam

total_words_in_ham

avg_words_in_spam_emails = total_words_in_spam // spam_emails_length.shape[0]
avg_words_in_ham_emails = total_words_in_ham // ham_emails_length.shape[0]

print("Average number of words in spam emails: ", avg_words_in_spam_emails)
print("Average number of words in non-spam emails: ", avg_words_in_ham_emails)

"""## Summing the Tokens Occuring in Spam"""

full_train_features.shape

# extract all emails that are spam messages
train_spam_tokens = full_train_features.loc[full_train_data.CATEGORY == 1]

train_spam_tokens.head()

train_spam_tokens.tail()

train_spam_tokens.shape

# Sum each column 
summed_spam_tokens = train_spam_tokens.sum(axis=0)

# LaPlace Smoothing 

'''

LaPlace Smoothing will prevent Zero Probability from occuring
if a word does not exist, we will add 1 so it has AT LEAST a likelihood of 1/n

'''
summed_spam_tokens += 1

summed_spam_tokens.head()

"""## Summing the Tokens Occuring in Non-Spam"""

# extract all emails that are non-spam messages
train_ham_tokens = full_train_features.loc[full_train_data.CATEGORY == 0]

train_ham_tokens.head()

train_ham_tokens.shape

summed_ham_tokens = train_ham_tokens.sum(axis=0)

summed_ham_tokens += 1

summed_ham_tokens.tail()

"""## P(Token | Spam) - Probability of Token Occuring Given that Email is Spam"""

'''

because we implemented LaPlace Smoothing by adding 1, we must also smooth our 
word counts by adding the VOCAB_SIZE. 

increase of 1 in summed tokens = increase of 1 fold of VOCAB_SIZE in total word count
'''

smoothed_spam_wordcount = total_words_in_spam + VOCAB_SIZE

'''
probability of word given that 
it is spam = total spam token count / total words in spam emails
'''

probs_token_spam = summed_spam_tokens / smoothed_spam_wordcount

probs_token_spam[:5]

# probability of all should = 1

probs_token_spam.sum()

"""## P(Token | Non-Spam) - Probability of Token Occuring Given that Email is Not Spam"""

'''

because we implemented LaPlace Smoothing by adding 1, we must also smooth our 
word counts by adding the VOCAB_SIZE. 

increase of 1 in summed tokens = increase of 1 fold of VOCAB_SIZE in total word count
'''

smoothed_ham_wordcount = total_words_in_ham + VOCAB_SIZE

'''
probability of word given that 
it is ham = total ham token count / total words in ham emails
'''

probs_token_ham = summed_ham_tokens / smoothed_ham_wordcount

probs_token_ham[:5]

probs_token_ham.sum()

"""## P(Token) - Probability of Token Occuring"""

prob_tokens_all = full_train_features.sum(axis=0) / total_words

prob_tokens_all

prob_tokens_all.head()

prob_tokens_all.sum()

probs_token_spam



"""# Save the Trained Model """

np.savetxt(TOKEN_SPAM_PROB_FILE,
           probs_token_spam)

np.savetxt(TOKEN_HAM_PROB_FILE,
           probs_token_ham)

np.savetxt(TOKEN_ALL_PROB_FILE,
           prob_tokens_all)

"""# Prepare Test Data"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# full_test_data = make_full_matrix(sparse_matrix=sparse_test_data,
#                                    num_of_words=VOCAB_SIZE)

full_test_data.head()

full_test_data.tail()

X_test = full_test_data.loc[:, full_test_data.columns != 'CATEGORY']

y_test = full_test_data.CATEGORY

y_test

np.savetxt(TEST_FEATURE_MATRIX,
           X_test)

np.savetxt(TEST_TARGET_MATRIX,
           y_test)