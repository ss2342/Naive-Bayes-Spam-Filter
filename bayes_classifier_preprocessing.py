# -*- coding: utf-8 -*-
"""Bayes Classifier - Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JW12BGLgqkrFNE9-LQ2wmN-9e8GT8A5C

#### Notebook Imports
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
from os import walk
from os.path import join

import pandas as pd
import matplotlib.pyplot as plt

import numpy as np


# for NLP
import nltk
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


# use for removing HTML tags
from bs4 import BeautifulSoup

# use to craete wordclouds
from wordcloud import WordCloud
from PIL import Image # we will use it to convert image to grayscale and then to array of RGB values

# use for creating training and test data
from sklearn.model_selection import train_test_split

# %matplotlib inline

pip install beautifulsoup4

"""#### Constants"""

EXAMPLE_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/01_Processing/practice_email.txt' # relative path of file

# spam emails
SPAM_1_PATH = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/01_Processing/spam_assassin_corpus/spam_1'
SPAM_2_PATH = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/01_Processing/spam_assassin_corpus/spam_2'

# normal emails
EASY_NONSPAM_1_PATH = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/01_Processing/spam_assassin_corpus/easy_ham_1'
EASY_NONSPAM_2_PATH = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/01_Processing/spam_assassin_corpus/easy_ham_2'

SPAM_CATEGORY = 1 # is SPAM
HAM_CATEGORY = 0 # is NOT SPAM
VOCAB_SIZE = 2500 # size of vocab list we want to use

# save files 
DATA_JSON_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/01_Processing/email-text-data.json'
WORD_ID_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/01_Processing/word-by-id.csv'

TRAINING_DATA_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/02_Training/train-data.txt'
TEST_DATA_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/02_Training/test-data.txt'

# whale wordcloud
WHALE_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/01_Processing/wordcloud_resources/whale-icon.png'

# skull wordcloud
SKULL_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/01_Processing/wordcloud_resources/skull-icon.png'


# Thumbs up wordcloud for non-spam mail
THUMBS_UP_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/01_Processing/wordcloud_resources/thumbs-up.png'

# Thumbs up wordcloud for non-spam mail
THUMBS_DOWN_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/01_Processing/wordcloud_resources/thumbs-down.png'

# font file
FONT_FILE = '/content/drive/MyDrive/Machine Learning /Naive Bayes Spam Filter/SpamData/01_Processing/wordcloud_resources/OpenSansCondensed-Bold.ttf'

"""#### Reading Files"""

# set up pipeline to read file
stream = open(EXAMPLE_FILE, encoding='latin-1') # open file
message = stream.read() # read file
stream.close() # close file after reading file

print(type(message))
print(message)

stream = open(EXAMPLE_FILE, encoding='latin-1') # open file

is_body = False
lines = [] # lines list, holds all lines

# header and body of email separated by new line
# therefore, change is_body to True, if there is a line that is just new line char
for line in stream:

    # if is_body = True, add line to lines list
    if is_body:
        lines.append(line)

    # if line is = new line char, set is_body to True
    elif line == '\n':
        is_body = True

stream.close() # close file after reading file

# join all individual entries in list with a new line character
# make it more readable
email_body = '\n'.join(lines)

print(email_body)

import sys
sys.getfilesystemencoding() # get default encoding on our machine

"""#### Generator Functions"""

# example

def generate_squares(N):
    for myNumber in range(N):
        yield myNumber ** 2
        
# yield remembers the last point where the function left off and starts from there

generate_squares(3)

for i in generate_squares(5):
    print(i, end=' -> ')

for i in generate_squares(5):
    print(i, end=' -> ')

"""#### Email body Extraction"""

def email_body_generator(path):

    '''
    our os will walk through the file directory tree and
    YIELD us a tuple containing:
       
    the root of the directories, 
    the names of directories,
    and the individual file names 
       '''
    for root, dirnames, filenames in walk(path):
        for file_name in filenames:
          
            # generate our file path by JOINING the root and file_name
            # root/file_name
            filepath = join(root, file_name) # find path of file using root dir and filename

            stream = open(filepath, encoding='latin-1') # open file

            is_body = False
            lines = [] # lines list, holds all lines

            # header and body of email separated by new line
            # therefore, change is_body to True, if there is a line that is just new line char
            for line in stream:
              
              # if is_body = True, add line to lines list
              if is_body:
                lines.append(line)

              # if line is = new line char, set is_body to True
              elif line == '\n':
                is_body = True

            stream.close() # close file after reading file

            # join all individual entries in list with a new line character
            # make it more readable
            email_body = '\n'.join(lines)

            yield file_name, email_body

# df = dataframe

'''
df_from_directory(path, classification)

  path: file path
  classification: whether file is SPAM or NOT SPAM

  return value: pandas dataframes containing all the rows, indexed by the 
                row name which is the name of the file 
'''
def df_from_directory(path, classification):

    rows = []
    row_names = []

    # calling generator function in loop
    # provide path to generator function to get file_name and email_body
    for file_name, email_body in email_body_generator(path):

        # create a dictionary from the values that our generator gives us
        # each time loop runs, rows list gets appended with a dictionary
        # dictionary contains 
        #   email body (Message) 
        #   SPAM or NOT SPAM (Classification)
        rows.append({'MESSAGE': email_body, 
                     'CATEGORY': classification})
        
        # each time loop runs, row_names list gets appended with name of file
        row_names.append(file_name)
        
    # return a pandas dataframe 
    return pd.DataFrame(rows, index=row_names)

# all SPAM emails

spam_emails = df_from_directory(SPAM_1_PATH, SPAM_CATEGORY)

# add SPAM_2 to spam emails as well
spam_emails = spam_emails.append(df_from_directory(SPAM_2_PATH, SPAM_CATEGORY))
spam_emails.head()

spam_emails.shape

non_spam_emails = df_from_directory(EASY_NONSPAM_1_PATH, HAM_CATEGORY)
non_spam_emails = non_spam_emails.append(df_from_directory(EASY_NONSPAM_2_PATH, HAM_CATEGORY))

non_spam_emails.shape

# combine both span emails dataframe and non-spam emails dataframe

data = pd.concat([spam_emails, non_spam_emails])
print("Shape of entire dataframe is ", data.shape)
data.head()

data.tail()

'''
Next Few Steps...
    Extract relevant data(email bodies)
    Convert format from txt files to DataFrame
    Check for empty emails
    Check for null or missing values
'''

"""#### Data Cleaning Checking for Missing Values"""

# check if any message bodies are null

# data["MESSAGE"].isnull() will show message and T/F for all rows

# data["MESSAGE"].isnull().values will show just the T/F for all rows

# data["MESSAGE"].isnull().values.any() will return boolean if any of the values are missing
  # True if there are missing values
  # False if no missing values

data["MESSAGE"].isnull().values.any()

type("") # this does not mean null!

my_var = None

type(my_var) # this is null

"""#### Locate Empty Emails"""

# locate empty emails by first converting the message into a string


type(data.MESSAGE.str.len() == 0)

# T/F if there are ANY emails that are an empty string 
(data.MESSAGE.str.len() == 0).any()

# how many emails are there that are an empty string
(data.MESSAGE.str.len() == 0).sum()

data[data.MESSAGE.str.len() == 0].index

# cmds is a system file when unzipping a file

# find how many emails have NULL/None values 

data.MESSAGE.isnull().sum()

data.shape

"""#### Remove System File Entries from Dataframe"""

# drop all the system file entries
# access them by their name

# data.drop([nameOfFile])
# by setting inplace=True, we do not have to overwrite our data with new data var
data.drop(['cmds'], inplace=True)

data.shape

"""#### Add Document IDs to Track Emails in Dataset"""

# add a numerical id to each email (document)

# starts from 0 and goes to len(data) - 1
document_ids = range(0, len(data.index))

# create a new column called DOC_ID that stores document ids
data['DOC_ID'] = document_ids

# create a new column called called file name and set it to index of each email
data['FILE_NAME'] = data.index

# set dataframe index to be the DOC_ID
data.set_index("DOC_ID", inplace=True)
data.head()

data.tail()

"""#### Save to File Using Pandas"""

# save our data to JSON file
data.to_json(DATA_JSON_FILE)

"""#### Number of Spam Messages Visualised (Pie Charts)"""

# break down SPAM vs NOT SPAM

# 0 = NOT SPAM
# 1 = SPAM
data.CATEGORY.value_counts()

# store number of spam mails 
amount_of_spam = data.CATEGORY.value_counts()[1] # 1 = access second value in value_counts

# store number of non-spam mails
amount_of_not_spam = data.CATEGORY.value_counts()[0] # 0 = access first value in value_counts

# create pie chart using matplotlib

category_names = ['Spam', 'Legit Mail'] # list of category names
sizes = [amount_of_spam, amount_of_not_spam] # size of slices of pie
colors = ["#ef233c", "#065a60"]

plt.figure(figsize=(2, 2),
           dpi=227) # dpi = resolution
# plot out pie chart
plt.pie(x=sizes, 
        labels=category_names, 
        colors=colors, # set custom colors
        textprops={'fontsize':6}, # set font size
        startangle=90, # rotate chart 90 degrees to give it upright position
        autopct='%1.0f%%', # round to 1 decimal point
        explode=[0, 0.1]) # have spam wedge pop out from graph, list sets offset between two wedges

plt.style.use('dark_background')
plt.show()

# create donut chart

plt.figure(figsize=(2, 2),
           dpi=227) # dpi = resolution

plt.pie(x=sizes, 
        labels=category_names, 
        colors=colors,
        textprops={'fontsize':6}, 
        startangle=90,
        autopct='%1.0f%%', 
        pctdistance=0.7)

# draw circle
center_circle = plt.Circle((0,0), # circle origin = (0,0)
                           radius=0.6, # circle radius = 0.6
                           fc='black') # creates the black circle in the center

# get current axis, add the circle to our figure
# think this: black circle superimposed on the pie chart to create donut chart
plt.gca().add_artist(center_circle)

plt.style.use('dark_background')
plt.show()

# create donut chart with more than 2 categories

new_category_names = ['Spam', 'Legit Mail', 'Updates', 'Promotions']
sizes = [25, 43, 19, 22] # category_name : size
custom_colors = ['#fc427b', '#2C3A47', '#3B3B98', '#58B19F']
offset = [0.05, 0.05, 0.05, 0.05]

plt.figure(figsize=(2, 2),
           dpi=227) # dpi = resolution

plt.pie(sizes, 
        labels=new_category_names, 
        textprops={'fontsize':6}, 
        startangle=90,
        autopct='%1.0f%%', 
        colors=custom_colors, 
        pctdistance=0.7, 
        explode=offset)

# draw circle
center_circle = plt.Circle((0,0), radius=0.6, fc='black')
plt.gca().add_artist(center_circle)

plt.style.use('dark_background')
plt.show()

"""#### Natural Language Processing (NLP)

NLP Use Case Examples:
*   Search
*   Sentiment analysis
*   autocorrect
*   Siri/Alexa

We will use NLP for our Naive Bayes Spam Classifier

NLP will prepare our text for our learning algorithm (Preprocessing)

  How?

1.   Convert everything to lower case for consistency  
2.   Tokenising, meaning that we will split up individual words in a sentence
3.   Remove stop words (i.e: "the, of, am, on, I")
4.   Strip out HTML tags
5.   Word stemming (i.e. "going", "gone", "goes" will all be treated as "go"
6.   Remove Punctuation

##### Text Processing

##### Download the NLTK Resources (Tokenizer & Stopwords)

NLTK: Natural Langauge Tool Kit
"""

nltk.download('punkt')

# download stop words 

nltk.download('stopwords')

# download some sample texts

nltk.download('gutenberg')
nltk.download('shakespeare')

"""##### Tokenising"""

# split the msg into seperate words and turn into lowercase

msg = "All work and no play makes Jack a dull boy."
# word_tokenize(msg.lower())

# set is an unordered, immutable collection
# does not allow for duplicates
# therefore, useful for checking membership

stop_words = set(stopwords.words('english')) # set of all stop words in english

stop_words

# check for membership using "in" keyword
if 'lost' in stop_words:
    print("Found it")
else:
    print("not in here")

# make output lowercase, tokenize, and eliminate stop words

msg = "All work and no play makes Jack a dull boy. To be or not to be."

tokenized_lc_words = word_tokenize(msg.lower()) # tokenise and lowercase

# list of filtered word, that are tokenised, lower-cased, and are not stop words
filtered_words = [word for word in tokenized_lc_words if word not in stop_words]

filtered_words

"""##### Word Stems and Stemming"""

'''
word stemming
treat all derived and base words the same way

Examples:
fisher, fishing, fishy = fish
goes, going, gone = go
arguing, argue, arguement = argu (not a typo)

reduce all derived words down to the base word

'''

msg = "All work and no play makes Jack a dull boy. To be or not to be \
      Nobody expects the Spanish Inquisition!"
tokenized_lc_words = word_tokenize(msg.lower())

# de-facto Stemmer for English
stemmer = PorterStemmer()

# use SnowballStemmer to choose language

# list of filtered word, that are tokenised, lower-cased, are not stop words, and stemmed
filtered_words = [stemmer.stem(word) for word in tokenized_lc_words if word not in stop_words]

# stemmed words
print(filtered_words)

"""##### Removing Punctuation"""

# remove punctuation

msg = "All work and no play makes Jack a dull boy. To be or not to be. ???\
Nobody expects the Spanish Inquisition!"
tokenized_lc_words = word_tokenize(msg.lower())

stemmer = PorterStemmer()
# use SnowballStemmer to choose language

# list of filtered word, that are tokenised, lower-cased, are not stop words, stemmed, and punctuation removed
filtered_words = [stemmer.stem(word) for word in tokenized_lc_words 
                  if word not in stop_words and word.isalpha()]

# punctuation removed

print(filtered_words)

"""##### Removing HTML Tags from Emails"""

# view email in readable format using BeautifulSoup

soup = BeautifulSoup(data.at[2,'MESSAGE'], 'html.parser')
print(soup.prettify())

# Remove html tags using BeautifulSoup

soup.get_text() # remove html tags

"""##### Functions for Email Processing"""

'''
clean_message(message, stemmer=PorterStemmer(),
                 stop_words=set(stopwords.words('english'))

  message: REQUIRED
           email body

  stemmer: OPTIONAL
           which stemmer are you using
           default=PorterStemmer()
  
  stop_words: OPTIONAL
              set of stop words
              default = set of stop words in English

  return value: list of filtered words that are lowercased, stemmed, 
                and do not have any stop words or punctuation
                 
'''

def clean_message(message, stemmer=PorterStemmer(),
                 stop_words=set(stopwords.words('english'))):
    
    
    soup = BeautifulSoup(message, 'html.parser') # Remove HTML tags
    cleaned_text = soup.get_text() # save HTML-free email to new variable
    
    # Converts to Lower Case and Tokenizes(splits up) the message
    tokenized_lc_words = word_tokenize(cleaned_text.lower())
    
    # Removed stop words, punctuation, and gets word stems
    filtered_words = [stemmer.stem(word) for word in tokenized_lc_words 
                  if word not in stop_words and word.isalpha()]
    
    
    
    return filtered_words

clean_message(data.at[2, 'MESSAGE'])

"""##### Apply Cleaning and Tokenisation to all messages

###### Slicing Dataframs and Series & Creating Subsets
"""

data.at[2, 'MESSAGE'] # name-based lookup
data.iat[2, 2] # integer-based lookup

data.iloc[0:2] # output first two rows

# iloc also works on Series
first_emails = data.MESSAGE.iloc[0:3]

# apply clean_message function on first_emails Series
nested_lst = first_emails.apply(clean_message)

print(nested_lst[0]) # sublist within nested_lst

print(nested_lst)

# flatten a nested lst

def flatten_lst(nested_lst):


  flattened_lst = []

  for sublst in nested_lst:

    for item in sublst:

      flattened_lst.append(item)

  return flattened_lst

# flatten our nested list by adding individual items of each sublist to flattened list

flattened_lst = flatten_lst(nested_lst)

# could do this with list comprehension but loses readability with nested loops
# flattened_lst = [item for sublist in nested_lst for item in sublist]

len(flattened_lst)

# use apply() on all messages in the dataframe
nested_lst = data.MESSAGE.apply(clean_message)

nested_lst.head

nested_lst.tail

"""###### Using Logic to Slice Dataframes"""

data[data.CATEGORY == 1] # only return rows where Category = 1 (is Spam)

doc_ids_spam = data[data.CATEGORY == 1].index # store the indices of all Spam emails

doc_ids_ham = data[data.CATEGORY == 0].index # store the indices of all non-Spam emails

"""###### Subsetting a Series with an Index"""

type(doc_ids_ham)

type(nested_lst)

nested_lst_ham = nested_lst.loc[doc_ids_ham] # subset of all non-Spam emails

nested_lst_spam = nested_lst.loc[doc_ids_spam] # subset of all Spam emails

nested_lst_ham

nested_lst_spam

# calculate the total number of words in spam emails
spam_emails_words_len = len([item for sublst in nested_lst_spam for item in sublst])

# calculate the total number of words in non-spam emails
ham_emails_words_len = len([item for sublst in nested_lst_ham for item in sublst])

spam_emails_words_len

ham_emails_words_len

def find_most_common_words(nested_lst, top_n=10):

  flattened_lst = flatten_lst(nested_lst) # first flatten nested spam list
  series_of_words = pd.Series(flattened_lst) # convert to pandas Series

  return series_of_words.value_counts()[0:top_n] # return top n common words

# find 10 most common words in spam emails
find_most_common_words(nested_lst_spam, top_n=10)

# find 10 most common words in non-spam emails
find_most_common_words(nested_lst_ham, top_n=10)

"""##### Creating a Word Cloud"""

word_cloud = WordCloud().generate(email_body) # create word cloud from email body

plt.imshow(word_cloud) # display data as an image

plt.axis('off') # turn off axis

plt.show()

word_cloud = WordCloud().generate(email_body)

plt.imshow(word_cloud,
           interpolation='bilinear')

plt.axis('off')

plt.show()

# corpus of melville moby dick
example_corpus = nltk.corpus.gutenberg.words('melville-moby_dick.txt')

len(example_corpus)

word_lst = [''.join(word) for word in example_corpus]

word_lst

# display the entire novel as a string

'''
we need to convert to string as wordCloud expects a simple string argument
'''
novel_as_string = ' '.join(word_lst)

novel_as_string

icon = Image.open(WHALE_FILE) # open our image using PIL.Image

# our blank canvas
# will result in a white silhoutte of image
image_mask = Image.new(mode='RGB', # rgb mode
          size=icon.size, # size = size of our original image 
          color=(255, 255, 255)) # color = white

# paste image on to blank canvas
image_mask.paste(icon, box=icon)

# convert image object to rgb array
rgb_array = np.array(image_mask)

word_cloud = WordCloud(mask=rgb_array, # mask is our rgb array 
                       background_color='white', # background color = white
                       max_words=400) 

word_cloud.generate(novel_as_string)

plt.figure(figsize=([16, 8]),
           dpi=227) # dpi = resolution

plt.imshow(word_cloud,
           interpolation='bilinear')

plt.axis('off')

plt.show()

rgb_array.shape # whale icon is 1024 x 2048, 3 columns (r g b)

# corpus of hamlet shakespeare 
hamlet_corpus = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')

len(hamlet_corpus)

hamlet_words_lst = [''.join(word) for word in hamlet_corpus]

hamlet_words_str = ' '.join(hamlet_words_lst)

hamlet_words_str

print(hamlet_words_str)

new_icon = Image.open(SKULL_FILE) # open our image using PIL.Image

# our blank canvas
# will result in a white silhoutte of image
new_image_mask = Image.new(mode='RGB', # rgb mode
          size=new_icon.size, # size = size of our original image 
          color=(255, 255, 255)) # color = white

# paste image on to blank canvas
new_image_mask.paste(new_icon, box=new_icon)

# convert image object to rgb array
new_rgb_array = np.array(new_image_mask)

new_word_cloud = WordCloud(mask=new_rgb_array, # mask is our rgb array 
                       background_color='black', # background color = white
                       max_words=600) 

new_word_cloud.generate(hamlet_words_str)

plt.figure(figsize=([16, 8]),
           dpi=227) # dpi = resolution

plt.imshow(new_word_cloud,
           interpolation='bilinear')

plt.axis('off') # remove axis

plt.show()

"""###### Creating Word Clouds for Emails"""

# first flatten nested spam lists and non-spam lists

flattened_spam_lst = flatten_lst(nested_lst_spam)
flattened_ham_lst = flatten_lst(nested_lst_ham)

# next, convert the flattened lists into strings

spam_emails_str = ' '.join(flattened_spam_lst)
ham_emails_str = ' '.join(flattened_ham_lst)

'''
create_wordCloud(words_str, mask_img=None, mask_color=(255, 255, 255), 
                     background_color='black', max_words=200,
                     figsize=[16, 8], resolution=227,
                     interpolation='bilinear'):

  words_str: REQUIRED
             string of words to create word cloud out of

  mask_img: OPTIONAL
            image of mask word cloud should be 
            default=None, no mask

  mask_color: OPTIONAL
              rgb tuple of mask colors
              default=(255, 255, 255), white

  font_path: OPTIONAL
             file path of desired font
             default=None

  colormap: OPTIONAL
            matplotlib color scheme
            default=None (virdis)
  
  background_color: OPTIONAL
                    background color of word cloud
                    default=black
  
  max_words: OPTIONAL
             max number of words in word cloud
             default=200

  figsize: OPTIONAL
           dimensions of figure
           default=[16,8]

  resolution: OPTIONAL
              resolution of the graph (dpi)
              default=227
  
  interpolation: OPTIONAL
                 type of interpolation on image
                 default='bilinear'


  return value: None, display word cloud 
                 
'''



def create_wordCloud(words_str, mask_img=None, mask_color=(255, 255, 255), 
                     font_path=None, colormap=None, background_color='black', max_words=200,
                     figsize=[16, 8], resolution=227,
                     interpolation='bilinear'):
  
  new_icon = Image.open(mask_img) # open our image using PIL.Image

  # our blank canvas
  # will result in a white silhoutte of image
  new_image_mask = Image.new(mode='RGB', # rgb mode
            size=new_icon.size, # size = size of our original image 
            color=mask_color) # color = white

  # paste image on to blank canvas
  new_image_mask.paste(new_icon, box=new_icon)

  # convert image object to rgb array
  new_rgb_array = np.array(new_image_mask)

  new_word_cloud = WordCloud(font_path=font_path, # font that you want to use 
                             mask=new_rgb_array, # mask is our rgb array 
                        background_color=background_color, # background color = white
                        max_words=max_words,
                        colormap=colormap) 

  new_word_cloud.generate(words_str.upper())

  plt.figure(figsize=(figsize),
            dpi=resolution) # dpi = resolution

  plt.imshow(new_word_cloud,
            interpolation=interpolation)

  plt.axis('off') # remove axis

  plt.show()

create_wordCloud(spam_emails_str,
                 mask_img=THUMBS_DOWN_FILE,
                 font_path=FONT_FILE,
                 colormap='gist_heat',
                 background_color='white',
                 max_words=2000)

create_wordCloud(ham_emails_str, 
                 mask_img=THUMBS_UP_FILE,
                 font_path=FONT_FILE,
                 colormap='gist_heat',
                 background_color='white',
                 max_words=2000)

"""##### Generate Vocabulary & Dictionary"""

# flatten our nested list
flat_stemmed_lst = flatten_lst(nested_lst)

unique_words = pd.Series(flat_stemmed_lst).value_counts()

# print total number of unique words
print("Number of unique words", unique_words.shape[0])

frequent_words_method1 = unique_words[:2500]

frequent_words_method1

# find the top 2500 most common words - method 2
frequent_words = find_most_common_words(nested_lst, top_n=2500)

frequent_words

# print the top 10 of the most frequent_words

frequent_words[:10]

"""
###### Create Vocabulary Dataframe with a WORD_ID"""

word_ids = list(range(0, VOCAB_SIZE)) # create ids of words ranging from 0 to 2500

# create a dataframe with a word and its index
vocab = pd.DataFrame({'VOCAB_WORD': frequent_words.index.values}, # access the words by their indices
                     index=word_ids) # explicitly set index to be word_ids

vocab.index.name = 'WORD_ID' # set column name of indices

vocab.VOCAB_WORD.values[0]

"""##### Save Words By ID to CSV file"""

# save our data to CSV file

vocab.to_csv(WORD_ID_FILE, 
            index=vocab.index.name, # index = name of index in vocab dataframe
            header=vocab.VOCAB_WORD.name) # header = words in vocab dataframe

# Check if word is in vocab:
# use set!!!

def check_if_word_exists(word, collection_of_words):

  return word in set(collection_of_words)

print(check_if_word_exists('machine', vocab.VOCAB_WORD))
print(check_if_word_exists('learning', vocab.VOCAB_WORD))
print(check_if_word_exists('fun', vocab.VOCAB_WORD))
print(check_if_word_exists('learn', vocab.VOCAB_WORD))
print(check_if_word_exists('data', vocab.VOCAB_WORD))
print(check_if_word_exists('science', vocab.VOCAB_WORD))
print(check_if_word_exists('app', vocab.VOCAB_WORD))
print(check_if_word_exists('brewery', vocab.VOCAB_WORD))

nested_lst.values[0]

# Find the Email with the Most Number of words

emails_len_lst = [len(email) for email in nested_lst] # list of email lengths

longest_email_len = max(emails_len_lst) # find the max length in length list

longest_email_index = emails_len_lst.index(longest_email_len) # find the index associated with that length

# you could also use argmax to find the index
# longest_email_index = np.argmax(emails_len_lst)


longest_email_body = nested_lst[longest_email_index] # print list of words at longest_email_index

print('The longest email is', longest_email_len, 'characters long')
print('The longest email occus at index: ', longest_email_index)
print('The longest email is: ', longest_email_body)

data.at[np.argmax(emails_len_lst), 'MESSAGE'] # locate the longest email in data dataframe

"""##### Generate Features & a Sparse Matrix

###### Creating a Dataframe with one word per column
"""

type(nested_lst)

# first convert our series into a list
converted_nested_lst = nested_lst.to_list()

# next convert to pandas Dataframe
word_columns_df = pd.DataFrame.from_records(converted_nested_lst)

word_columns_df

word_columns_df.shape # width=totalEmails x length(columns)=lengthOfLongestEmail

"""##### Splitting the Data into a Training and Test Dataset"""

# training data = 70%, test data = 30%
# seed value of 42
# features = dataframe of all our words
# target variable = category column in data dataframe
# X_train and y_train match up
# X_tets and y_test match up
X_train, X_test, y_train, y_test = train_test_split(word_columns_df,
                                                    data.CATEGORY,
                                                    test_size=0.3,
                                                    random_state=42)

# add index name to X_train and X_test
X_train.index.name = X_test.index.name = 'DOC_ID'

X_train.head()

X_train.index[3]

# X_train.index[0]

len(X_train.iloc[0])

# print(X_train.iloc[0][1])
# i = 0
# for item in X_train.iloc[0]:

#   print (item)

#   if item == None:

#     break

i = 0

while X_train.iloc[0][i] != None:

  print (X_train.iloc[0][i])

  i+=1

y_train.head()

"""##### Create a Sparse Matrix for the Training Data"""

# use X_train, y_train, and vocab words

# create a way to index inside our vocab dataframe
word_index = pd.Index(vocab.VOCAB_WORD)

type(X_train)

'''
def make_sparse_matrix(dataframe, indexed_words, labels)

  dataframe: REQUIRED
             pandas dataframe

  indexed_words: REQUIRED
                 index of words ordered by word_id
           
  
  labels: REQUIRED
          which category to be classified in

  return value: a sparse matrix as a dataframe 
  recall, a sparse matrix will only contain words that 
                 
'''


def make_sparse_matrix(dataframe, indexed_words, labels):


  num_of_rows = dataframe.shape[0] # number of rows in input dataframe
  num_of_columns = dataframe.shape[1] # number of columns in input dataframe
  word_set = set(indexed_words) # set of unique words to check for membership 

  dict_lst = []

  for i in range(num_of_rows):

    for j in range(num_of_columns):

      word = dataframe.iat[i , j] # retrive word at given row and column 

      if word in word_set: # if word is in the word set
 
        doc_id = dataframe.index[i] # doc id is the corresponding index of row in dataframe
        word_id = indexed_words.get_loc(word) # word id = index of word in indexed words
        category = labels.at[doc_id] # spam vs not spam obtained from labels dataframe (y_train)

        # store retrieved information in dictionary
        item = {
            'LABEL': category,
            'DOC_ID': doc_id, 
            'OCCURENCE': 1,
            'WORD_ID': word_id
        }

        dict_lst.append(item)

  return pd.DataFrame(dict_lst) # convert dict_lst into pandas dataframe

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# sparse_train_dataframe = make_sparse_matrix(X_train, word_index, y_train)

sparse_train_dataframe

sparse_train_dataframe.head()

sparse_train_dataframe.info()

sparse_train_dataframe.tail()

sparse_train_dataframe.shape

"""##### Combine Occurences with the Pandas groupby() Method"""

# for each doc_id (email), group together word_id and labels and show corresponding occurences

train_grouped = sparse_train_dataframe.groupby(['DOC_ID', 'WORD_ID', 'LABEL']).sum()

train_grouped.head()

vocab.at[0, 'VOCAB_WORD']

"""The first word at doc_id = 0 corresponds to the word "http" in the vocab dataframe and it occurs three times."""

data.MESSAGE[0]

train_grouped.tail()

train_grouped = train_grouped.reset_index() # will have doc_id show on every row
train_grouped.head()

train_grouped.tail()

vocab.at[343, 'VOCAB_WORD']

data.MESSAGE[5795]

train_grouped.shape # reduced our dataset by 40%

"""###### Save Training File as .txt file"""

# use numpy to save to txt
np.savetxt(TRAINING_DATA_FILE, # location to save to
           train_grouped, # data
           fmt='%d') # format of data, hence %d

train_grouped.columns # column names

train_grouped

"""##### Create a Sparse Matrix for the Test Data"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# sparse_test_dataframe = make_sparse_matrix(X_test, word_index, y_test)

# combine occurences with the pandas groupby() method

test_grouped = sparse_test_dataframe.groupby(['DOC_ID', 'WORD_ID', 'LABEL']).sum()

test_grouped = sparse_test_dataframe.reset_index()

# use numpy to save to txt
np.savetxt(TEST_DATA_FILE, # location to save to
           test_grouped, # data
           fmt='%d') # format of data, hence %d

test_grouped

"""##### Preprocessing Subtelties"""

emails_in_txt = set(test_grouped.DOC_ID.unique())
emails_in_test = set(X_test.index.unique())

len(emails_in_test)

count = len(emails_in_test) - len(emails_in_txt)

print(emails_in_test - emails_in_txt) # which emails were excluded 
print("Number of emails excluded in txt file is: ", count)

data.MESSAGE[45]

"""All the files that were excluded seem to be complete gibberish. Let's try to actually open the files and see what their contents are. A possible hypothesis for why these file contents are such is that maybe our encoding is incorrect."""

data.loc[45] # find file name

"""When you open the file you see a one-to-one match, meaning that indeed the file is gibberish and that there is nothing wrong with the encoding."""

clean_message(message=data.at[45, 'MESSAGE'])

"""Because we are checking for membership in our clean_message function to only include words that are in the top 2500 most common words, these emails that are nonsensical do not get included in our sparse matrix as they contain no common words in their email bodies."""